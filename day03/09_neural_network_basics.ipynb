{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# 9강: 인공신경망 개념과 프레임워크 설치\n",
    "\n",
    "## 🎯 학습 목표\n",
    "- 인공신경망의 기본 개념과 동작 원리 완전 이해\n",
    "- 퍼셉트론부터 다층 구조까지 수학적 원리 파악\n",
    "- AI 프레임워크 비교 및 PyTorch 설치 환경 구축\n",
    "- 딥러닝 학습을 위한 기초 지식 습득\n",
    "\n",
    "---\n",
    "\n",
    "## 📚 강의 개요 (60분)\n",
    "1. 인공신경망 개념 (25분)\n",
    "2. 퍼셉트론과 다층 구조 (25분)  \n",
    "3. AI 프레임워크 설치 (10분)\n",
    "\n",
    "> **다음 강의 예고**: 10강에서는 PyTorch로 실제 신경망 모델을 구현하고 순전파/역전파를 학습합니다!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 1. 인공신경망 개념\n",
    "\n",
    "### 1.1 인공신경망의 탄생 배경\n",
    "\n",
    "**🧠 인간의 뇌에서 영감을 받다**\n",
    "\n",
    "1943년 맥컬록과 피츠가 처음으로 인공 뉴런을 수학적으로 모델링했습니다. \n",
    "인간의 뇌가 어떻게 학습하고 기억하는지에 대한 호기심에서 시작되었습니다.\n",
    "\n",
    "**🔬 뇌과학의 발견들:**\n",
    "- 뉴런은 **전기 신호**로 정보를 전달\n",
    "- **시냅스**를 통해 뉴런들이 연결\n",
    "- **반복 학습**을 통해 시냅스 연결이 강화됨\n",
    "- **병렬 처리**로 복잡한 작업 수행\n",
    "\n",
    "### 1.2 생물학적 뉴런 vs 인공 뉴런\n",
    "\n",
    "**🧠 생물학적 뉴런의 구조:**\n",
    "\n",
    "```\n",
    "수상돌기 → 세포체 → 축삭 → 시냅스 → 다음 뉴런\n",
    " (입력)   (처리)   (전달)  (연결점)\n",
    "```\n",
    "\n",
    "**⚡ 뉴런의 동작 과정:**\n",
    "1. **수상돌기**: 여러 뉴런으로부터 화학 신호 수신\n",
    "2. **세포체**: 받은 신호들을 합산하여 임계값 비교\n",
    "3. **축삭**: 임계값 초과 시 전기 신호(활동전위) 발생\n",
    "4. **시냅스**: 화학물질(신경전달물질) 방출로 신호 전달\n",
    "\n",
    "**🤖 인공 뉴런의 구조:**\n",
    "\n",
    "```\n",
    "입력 x₁ ──w₁──┐\n",
    "입력 x₂ ──w₂──┤\n",
    "입력 x₃ ──w₃──┼──→ Σ ──→ f() ──→ 출력 y\n",
    "    ⋮           │\n",
    "입력 xₙ ──wₙ──┘\n",
    "        +\n",
    "      bias b\n",
    "```\n",
    "\n",
    "**📊 수학적 표현:**\n",
    "\n",
    "```\n",
    "net = Σ(wᵢ × xᵢ) + b = w₁x₁ + w₂x₂ + ... + wₙxₙ + b\n",
    "y = f(net)\n",
    "```\n",
    "\n",
    "**🔍 구성 요소 상세 설명:**\n",
    "\n",
    "| 생물학적 뉴런 | 인공 뉴런 | 역할 |\n",
    "|---------------|-----------|------|\n",
    "| **수상돌기** | **입력 x** | 외부 신호 수신 |\n",
    "| **시냅스 강도** | **가중치 w** | 신호의 중요도 조절 |\n",
    "| **세포체 임계값** | **편향 b** | 뉴런의 활성화 민감도 |\n",
    "| **활동전위** | **활성화함수 f** | 출력 신호 결정 |\n",
    "| **축삭** | **출력 y** | 다음 층으로 신호 전달 |\n",
    "\n",
    "### 1.3 인공신경망의 핵심 아이디어\n",
    "\n",
    "**💡 핵심 개념들:**\n",
    "\n",
    "1. **연결주의(Connectionism)**\n",
    "   - 단순한 처리 단위들이 연결되어 복잡한 행동 창발\n",
    "   - \"지능 = 간단한 계산의 대규모 병렬 처리\"\n",
    "\n",
    "2. **학습 가능성(Learnability)**\n",
    "   - 경험(데이터)을 통해 성능 향상\n",
    "   - 가중치 조정으로 원하는 출력 생성\n",
    "\n",
    "3. **분산 표현(Distributed Representation)**\n",
    "   - 정보가 여러 뉴런에 분산되어 저장\n",
    "   - 일부 뉴런 손상에도 전체 기능 유지\n",
    "\n",
    "### 1.4 인공신경망 vs 전통적 프로그래밍\n",
    "\n",
    "**🔄 패러다임의 전환:**\n",
    "\n",
    "**전통적 프로그래밍:**\n",
    "```\n",
    "입력 + 프로그램 → 출력\n",
    "(명시적 규칙, 로직 기반)\n",
    "```\n",
    "\n",
    "**인공신경망:**\n",
    "```\n",
    "입력 + 출력 → 프로그램(모델)\n",
    "(데이터 기반 학습, 패턴 인식)\n",
    "```\n",
    "\n",
    "**📊 차이점 비교:**\n",
    "\n",
    "| 특성 | 전통적 프로그래밍 | 인공신경망 |\n",
    "|------|-------------------|------------|\n",
    "| **문제 해결 방식** | 규칙 기반 | 패턴 학습 |\n",
    "| **코드 작성** | 직접 로직 구현 | 데이터로 학습 |\n",
    "| **복잡성 처리** | 제한적 | 뛰어남 |\n",
    "| **해석 가능성** | 높음 | 낮음 |\n",
    "| **일반화 능력** | 제한적 | 뛰어남 |\n",
    "\n",
    "### 1.5 인공신경망이 강력한 이유\n",
    "\n",
    "**🎯 왜 지금 인공신경망이 주목받을까요?**\n",
    "\n",
    "1. **범용 근사기(Universal Approximator)**\n",
    "   - 이론적으로 어떤 연속 함수든 근사 가능\n",
    "   - 복잡한 입출력 관계를 자동으로 학습\n",
    "\n",
    "2. **자동 특성 추출**\n",
    "   - 원본 데이터에서 중요한 패턴을 자동 발견\n",
    "   - 사람이 생각하지 못한 특성도 찾아냄\n",
    "\n",
    "3. **확장성(Scalability)**\n",
    "   - 데이터가 많을수록 성능 향상\n",
    "   - 컴퓨팅 파워 증가에 따라 모델 크기 확장 가능\n",
    "\n",
    "4. **전이 학습(Transfer Learning)**\n",
    "   - 한 분야에서 학습한 지식을 다른 분야에 적용\n",
    "   - 적은 데이터로도 좋은 성능 달성\n",
    "\n",
    "### 1.6 인공신경망의 한계와 고려사항\n",
    "\n",
    "**⚠️ 알아야 할 한계들:**\n",
    "\n",
    "1. **블랙박스 문제**\n",
    "   - 내부 동작 원리 이해 어려움\n",
    "   - 의료, 금융 등 설명 가능성이 중요한 분야에서 제약\n",
    "\n",
    "2. **데이터 의존성**\n",
    "   - 좋은 데이터 없이는 좋은 모델 불가능\n",
    "   - 편향된 데이터는 편향된 결과 생성\n",
    "\n",
    "3. **계산 비용**\n",
    "   - 학습에 많은 시간과 컴퓨팅 자원 필요\n",
    "   - GPU, TPU 등 전용 하드웨어 필요\n",
    "\n",
    "4. **과적합 위험**\n",
    "   - 훈련 데이터에만 특화될 가능성\n",
    "   - 새로운 데이터에 대한 일반화 실패\n",
    "\n",
    "**🤔 언제 사용해야 할까요?**\n",
    "\n",
    "✅ **신경망을 사용하는 것이 좋은 경우:**\n",
    "- 이미지, 음성, 텍스트 등 **비정형 데이터**\n",
    "- 패턴이 복잡하고 **비선형적**인 문제\n",
    "- **대용량 데이터**가 사용 가능\n",
    "- **높은 정확도**가 핵심 요구사항\n",
    "- 문제의 **규칙을 명시하기 어려운** 경우\n",
    "\n",
    "❌ **전통적 방법이 더 나은 경우:**\n",
    "- **작은 데이터셋** (1,000개 미만)\n",
    "- **선형 관계**가 명확한 문제\n",
    "- **해석 가능성**이 매우 중요\n",
    "- **빠른 추론**이 필요 (실시간 시스템)\n",
    "- **개발 비용과 시간**이 제한적\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 2. 퍼셉트론과 다층 구조\n",
    "\n",
    "### 2.1 퍼셉트론의 탄생과 의미\n",
    "\n",
    "**⚡ 퍼셉트론 = 최초의 학습하는 기계**\n",
    "\n",
    "1957년 프랭크 로젠블라트(Frank Rosenblatt)가 개발한 **최초의 학습 가능한 인공 뉴런**입니다.\n",
    "당시 \"기계가 스스로 학습할 수 있다\"는 혁명적인 아이디어였습니다.\n",
    "\n",
    "**📚 역사적 의미:**\n",
    "- 최초의 **연결주의** 모델\n",
    "- **학습 알고리즘**을 가진 인공 뉴런\n",
    "- 현대 딥러닝의 **출발점**\n",
    "- 1960년대 첫 번째 AI 붐을 이끌어냄\n",
    "\n",
    "### 2.2 퍼셉트론의 수학적 원리\n",
    "\n",
    "**🔧 퍼셉트론의 구조:**\n",
    "\n",
    "```\n",
    "x₁ ──w₁──┐\n",
    "x₂ ──w₂──┤\n",
    "x₃ ──w₃──┼──→ net = Σ(wᵢxᵢ) + b ──→ f(net) ──→ y\n",
    "  ⋮      │\n",
    "xₙ ──wₙ──┘\n",
    "     +\n",
    "    bias b\n",
    "```\n",
    "\n",
    "**📊 수학적 정의:**\n",
    "\n",
    "```\n",
    "net = w₁x₁ + w₂x₂ + ... + wₙxₙ + b = Σ(wᵢxᵢ) + b\n",
    "\n",
    "출력 y = f(net) = {\n",
    "    1  if net ≥ 0\n",
    "    0  if net < 0\n",
    "}\n",
    "\n",
    "또는 y = sign(net) = {\n",
    "    +1  if net ≥ 0\n",
    "    -1  if net < 0\n",
    "}\n",
    "```\n",
    "\n",
    "**🎯 기하학적 해석:**\n",
    "\n",
    "퍼셉트론은 **n차원 공간을 두 영역으로 나누는 초평면**을 찾습니다.\n",
    "\n",
    "- **2차원**: 직선 `w₁x₁ + w₂x₂ + b = 0`\n",
    "- **3차원**: 평면 `w₁x₁ + w₂x₂ + w₃x₃ + b = 0`\n",
    "- **n차원**: 초평면 `Σ(wᵢxᵢ) + b = 0`\n",
    "\n",
    "### 2.3 퍼셉트론 학습 알고리즘\n",
    "\n",
    "**💡 로젠블라트의 학습 규칙:**\n",
    "\n",
    "```\n",
    "1. 가중치를 랜덤하게 초기화\n",
    "2. 각 훈련 샘플에 대해:\n",
    "   a) 예측값 계산: ŷ = sign(Σ(wᵢxᵢ) + b)\n",
    "   b) 오류 계산: error = y - ŷ\n",
    "   c) 가중치 업데이트:\n",
    "      wᵢ = wᵢ + η × error × xᵢ\n",
    "      b = b + η × error\n",
    "3. 모든 샘플이 올바르게 분류될 때까지 반복\n",
    "```\n",
    "\n",
    "**⚙️ 핵심 매개변수:**\n",
    "- **η (학습률)**: 가중치 업데이트 크기 조절 (보통 0.01~0.1)\n",
    "- **error**: 실제값과 예측값의 차이\n",
    "- **수렴 조건**: 모든 샘플이 올바르게 분류되면 학습 종료\n",
    "\n",
    "### 2.4 선형 분리 가능성 문제\n",
    "\n",
    "**✅ 퍼셉트론이 해결할 수 있는 문제:**\n",
    "\n",
    "**1. AND 게이트**\n",
    "```\n",
    "x₁  x₂  →  출력  |  기하학적 해석\n",
    "0   0   →   0    |     (0,0) ●\n",
    "0   1   →   0    |     (0,1) ●  ──── 분리선\n",
    "1   0   →   0    |     (1,0) ●\n",
    "1   1   →   1    |           (1,1) ○\n",
    "```\n",
    "**해**: w₁=0.5, w₂=0.5, b=-0.7\n",
    "\n",
    "**2. OR 게이트**\n",
    "```\n",
    "x₁  x₂  →  출력  |  기하학적 해석\n",
    "0   0   →   0    |     (0,0) ●\n",
    "0   1   →   1    |           (0,1) ○\n",
    "1   0   →   1    |  분리선 ──── (1,0) ○\n",
    "1   1   →   1    |           (1,1) ○\n",
    "```\n",
    "**해**: w₁=0.5, w₂=0.5, b=-0.2\n",
    "\n",
    "**❌ 퍼셉트론이 해결할 수 없는 문제:**\n",
    "\n",
    "**3. XOR 게이트 (배타적 논리합)**\n",
    "```\n",
    "x₁  x₂  →  출력  |  기하학적 해석\n",
    "0   0   →   0    |     (0,0) ●    (0,1) ○\n",
    "0   1   →   1    |        \n",
    "1   0   →   1    |        ?직선으로 분리 불가능?\n",
    "1   1   →   0    |     (1,0) ○    (1,1) ●\n",
    "```\n",
    "\n",
    "**🤔 왜 XOR은 불가능한가?**\n",
    "\n",
    "XOR은 **선형 분리 불가능(Linearly Non-separable)** 문제입니다.\n",
    "\n",
    "**수학적 증명:**\n",
    "XOR를 만족하는 w₁, w₂, b가 존재한다고 가정하면:\n",
    "- (0,0) → 0: b < 0\n",
    "- (0,1) → 1: w₂ + b > 0  \n",
    "- (1,0) → 1: w₁ + b > 0\n",
    "- (1,1) → 0: w₁ + w₂ + b < 0\n",
    "\n",
    "이는 w₁ + w₂ < -b 그리고 w₁ > -b, w₂ > -b를 동시에 만족해야 하므로 **모순**입니다.\n",
    "\n",
    "### 2.5 다층 퍼셉트론(MLP)의 필요성\n",
    "\n",
    "**🌟 XOR 문제의 해결책: 층을 더 쌓자!**\n",
    "\n",
    "1969년 민스키와 페이퍼트가 지적한 퍼셉트론의 한계를 극복하기 위해 **다층 구조**가 등장했습니다.\n",
    "\n",
    "**📊 XOR 해결을 위한 다층 퍼셉트론 구조:**\n",
    "\n",
    "```\n",
    "입력층     은닉층       출력층\n",
    "x₁ ──┐   ┌─ h₁ ──┐\n",
    "      ├───┤        ├─── y\n",
    "x₂ ──┘   └─ h₂ ──┘\n",
    "\n",
    "h₁ = f(w₁₁x₁ + w₁₂x₂ + b₁)  # AND의 부정\n",
    "h₂ = f(w₂₁x₁ + w₂₂x₂ + b₂)  # OR\n",
    "y = f(v₁h₁ + v₂h₂ + c)       # 최종 조합\n",
    "```\n",
    "\n",
    "**🎯 XOR 해결 전략:**\n",
    "1. **은닉층 h₁**: (NOT AND) = NAND 게이트 학습\n",
    "2. **은닉층 h₂**: OR 게이트 학습  \n",
    "3. **출력층**: NAND AND OR = XOR 조합\n",
    "\n",
    "### 2.6 다층 신경망의 구조와 용어\n",
    "\n",
    "**🏗️ 신경망 계층 구조:**\n",
    "\n",
    "```\n",
    "입력층     은닉층1    은닉층2    출력층\n",
    "x₁ ──┐   ┌─ h₁₁ ──┐ ┌─ h₂₁ ──┐\n",
    "x₂ ──┼───┼─ h₁₂ ──┼─┼─ h₂₂ ──┼─── y₁\n",
    "x₃ ──┤   ├─ h₁₃ ──┤ ├─ h₂₃ ──┤\n",
    "xₙ ──┘   └─ h₁ₘ ──┘ └─ h₂ₚ ──┘    yₖ\n",
    "```\n",
    "\n",
    "**📋 중요한 용어들:**\n",
    "\n",
    "| 용어 | 영어 | 설명 |\n",
    "|------|------|------|\n",
    "| **층/계층** | Layer | 같은 레벨에 있는 뉴런들의 집합 |\n",
    "| **깊이** | Depth | 전체 층의 개수 (입력층 제외하기도 함) |\n",
    "| **너비** | Width | 각 층의 뉴런 개수 |\n",
    "| **가중치** | Weight | 연결 강도 (W 또는 θ로 표기) |\n",
    "| **편향** | Bias | 뉴런의 활성화 임계값 (b로 표기) |\n",
    "| **활성화** | Activation | 뉴런의 출력값 |\n",
    "\n",
    "### 2.7 범용 근사 정리 (Universal Approximation Theorem)\n",
    "\n",
    "**🎯 다층 신경망의 이론적 근거**\n",
    "\n",
    "**정리**: 하나의 은닉층을 가진 신경망은 충분한 뉴런이 있다면 **어떤 연속 함수든 원하는 정확도로 근사할 수 있다**.\n",
    "\n",
    "**📊 수학적 표현:**\n",
    "임의의 연속함수 f: [0,1]ⁿ → ℝ에 대해, ∀ε > 0에 대해 다음을 만족하는 신경망 F가 존재:\n",
    "```\n",
    "|f(x) - F(x)| < ε  for all x ∈ [0,1]ⁿ\n",
    "```\n",
    "\n",
    "**💡 중요한 함의:**\n",
    "- **표현력**: 이론적으로 모든 함수 표현 가능\n",
    "- **학습 가능성**: 적절한 알고리즘으로 학습 가능\n",
    "- **실용적 한계**: \"충분한 뉴런\"이 얼마나 많은지는 불분명\n",
    "\n",
    "### 2.8 깊은 신경망의 필요성\n",
    "\n",
    "**🏗️ 왜 더 깊게 쌓을까?**\n",
    "\n",
    "1개 은닉층으로 모든 함수를 근사할 수 있다면, 왜 더 깊은 네트워크가 필요할까요?\n",
    "\n",
    "**🎯 깊은 네트워크의 장점:**\n",
    "\n",
    "1. **효율성**: 같은 함수를 더 적은 뉴런으로 표현\n",
    "2. **계층적 특성 학습**: 저수준 → 고수준 특성 자동 추출\n",
    "3. **구조적 유도 편향**: 계층적 구조가 학습에 유리한 편향 제공\n",
    "\n",
    "**📊 구체적 예시:**\n",
    "\n",
    "| 함수 복잡도 | 1개 은닉층 | 깊은 네트워크 |\n",
    "|-------------|------------|---------------|\n",
    "| **패리티 함수** | O(2ⁿ) 뉴런 | O(n) 뉴런 |\n",
    "| **대칭 함수** | O(2ⁿ) 뉴런 | O(n²) 뉴런 |\n",
    "| **합성 함수** | 지수적 증가 | 선형적 증가 |\n",
    "\n",
    "**🔍 계층적 특성 학습 예시:**\n",
    "\n",
    "```\n",
    "이미지 분류에서:\n",
    "1층: 에지, 선 감지\n",
    "2층: 코너, 곡선 조합\n",
    "3층: 기본 도형 (삼각형, 원)\n",
    "4층: 부분 객체 (눈, 코, 입)\n",
    "5층: 전체 객체 (얼굴, 자동차)\n",
    "```\n",
    "\n",
    "### 2.9 활성화 함수의 수학적 이해\n",
    "\n",
    "**🔥 비선형성의 핵심**\n",
    "\n",
    "활성화 함수가 없다면 다층 신경망도 단순한 선형 변환의 조합이 됩니다.\n",
    "\n",
    "**수학적 증명:**\n",
    "```\n",
    "활성화 함수가 없는 2층 네트워크:\n",
    "h = W₁x + b₁\n",
    "y = W₂h + b₂ = W₂(W₁x + b₁) + b₂ = (W₂W₁)x + (W₂b₁ + b₂)\n",
    "\n",
    "결과: 단일 선형 변환과 동일!\n",
    "```\n",
    "\n",
    "**📊 주요 활성화 함수의 특성:**\n",
    "\n",
    "| 함수 | 수식 | 도함수 | 범위 | 장점 | 단점 |\n",
    "|------|------|--------|------|------|------|\n",
    "| **시그모이드** | σ(x)=1/(1+e⁻ˣ) | σ'(x)=σ(x)(1-σ(x)) | (0,1) | 부드러운 곡선 | 기울기 소실 |\n",
    "| **tanh** | tanh(x) | 1-tanh²(x) | (-1,1) | 0 중심 | 기울기 소실 |\n",
    "| **ReLU** | max(0,x) | {0 if x<0, 1 if x>0} | [0,∞) | 계산 빠름 | 죽은 뉴런 |\n",
    "| **Leaky ReLU** | max(0.01x,x) | {0.01 if x<0, 1 if x>0} | (-∞,∞) | 죽은 뉴런 해결 | 하이퍼파라미터 |\n",
    "\n",
    "이제 퍼셉트론부터 다층 신경망까지의 이론적 기초를 탄탄히 다졌습니다. 다음 단계에서는 이를 실제로 구현해보겠습니다!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 3. AI 프레임워크 설치\n",
    "\n",
    "### 3.1 딥러닝 프레임워크 선택\n",
    "\n",
    "**🤔 왜 여러 프레임워크가 존재할까요?**\n",
    "\n",
    "딥러닝이 발전하면서 다양한 요구사항을 만족하기 위해 여러 프레임워크가 개발되었습니다.\n",
    "\n",
    "**📊 주요 딥러닝 프레임워크 비교:**\n",
    "\n",
    "| 프레임워크 | 개발사 | 출시년도 | 특징 | 주요 사용처 |\n",
    "|------------|--------|----------|------|-------------|\n",
    "| **PyTorch** | Meta | 2016 | 동적 그래프, 직관적 | 연구, 프로토타이핑 |\n",
    "| **TensorFlow** | Google | 2015 | 정적 그래프, 안정적 | 배포, 프로덕션 |\n",
    "| **Keras** | - | 2015 | 고수준 API | 교육, 빠른 개발 |\n",
    "\n",
    "### 3.2 PyTorch를 선택하는 이유\n",
    "\n",
    "**🎯 이 강의에서 PyTorch를 사용하는 이유:**\n",
    "\n",
    "✅ **학습에 최적화된 특징:**\n",
    "- **동적 그래프**: 디버깅이 쉽고 직관적\n",
    "- **파이썬 중심**: 파이썬 문법과 매우 유사\n",
    "- **즉시 실행**: 코드 한 줄씩 바로 실행 가능\n",
    "- **풍부한 자료**: 튜토리얼과 예제가 많음\n",
    "\n",
    "✅ **현업에서 인정받는 이유:**\n",
    "- **연구 친화적**: 새로운 아이디어 빠른 구현\n",
    "- **커뮤니티**: Meta, 스탠포드 등 주요 연구기관 사용\n",
    "- **성능**: GPU 가속 및 분산 학습 지원\n",
    "- **생태계**: 다양한 확장 라이브러리 존재\n",
    "\n",
    "### 3.3 PyTorch 설치 가이드\n",
    "\n",
    "**💻 설치 전 준비사항:**\n",
    "\n",
    "1. **Python 버전 확인** (3.8 이상 권장)\n",
    "2. **운영체제 확인** (Windows, macOS, Linux)\n",
    "3. **GPU 사용 여부 결정** (CUDA 지원 그래픽카드)\n",
    "\n",
    "**🔧 설치 방법:**\n",
    "\n",
    "**방법 1: 공식 사이트 이용 (권장)**\n",
    "1. https://pytorch.org/get-started/locally/ 접속\n",
    "2. 환경에 맞는 설치 명령어 자동 생성\n",
    "3. 생성된 명령어 복사하여 실행\n",
    "\n",
    "**방법 2: 직접 명령어 입력**\n",
    "\n",
    "```bash\n",
    "# CPU 버전 (기본)\n",
    "pip install torch torchvision torchaudio\n",
    "\n",
    "# GPU 버전 (CUDA 11.8)\n",
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "# GPU 버전 (CUDA 12.1)\n",
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "```\n",
    "\n",
    "### 3.4 추가 라이브러리 설치\n",
    "\n",
    "**📦 함께 설치하면 좋은 라이브러리들:**\n",
    "\n",
    "```bash\n",
    "# 데이터 처리 및 시각화\n",
    "pip install numpy pandas matplotlib seaborn\n",
    "\n",
    "# 과학 계산\n",
    "pip install scikit-learn scipy\n",
    "\n",
    "# 딥러닝 유틸리티\n",
    "pip install torchsummary\n",
    "\n",
    "# 진행률 표시\n",
    "pip install tqdm\n",
    "\n",
    "# 노트북 환경\n",
    "pip install jupyter notebook\n",
    "```\n",
    "\n",
    "### 3.5 설치 확인 방법\n",
    "\n",
    "다음 강의(10강)에서 실제 설치를 확인하고 기본 사용법을 배울 예정입니다.\n",
    "\n",
    "**확인해볼 항목들:**\n",
    "- PyTorch 버전 확인\n",
    "- CUDA 사용 가능 여부\n",
    "- 기본 텐서 연산\n",
    "- GPU 메모리 상태\n",
    "\n",
    "### 3.6 개발 환경 추천\n",
    "\n",
    "**🛠️ 초보자를 위한 환경 설정 추천:**\n",
    "\n",
    "1. **Google Colab** (무료, 설치 불필요)\n",
    "   - 브라우저에서 즉시 사용 가능\n",
    "   - 무료 GPU 제공 (제한적)\n",
    "   - 파일 관리가 다소 불편\n",
    "\n",
    "2. **로컬 환경** (Anaconda + Jupyter)\n",
    "   - 안정적이고 빠른 개발\n",
    "   - 데이터 보안 유지\n",
    "   - 초기 설정이 복잡할 수 있음\n",
    "\n",
    "3. **VS Code** (전문 개발자용)\n",
    "   - 강력한 디버깅 기능\n",
    "   - 확장 프로그램 풍부\n",
    "   - 학습 곡선이 있음\n",
    "\n",
    "**💡 권장 학습 순서:**\n",
    "1. **Google Colab**로 시작 → 빠른 체험\n",
    "2. **로컬 환경** 구축 → 본격적인 학습\n",
    "3. **전문 도구** 도입 → 실무 수준 개발\n",
    "\n",
    "---\n",
    "\n",
    "## 📚 9강 정리\n",
    "\n",
    "### 🎯 오늘 학습한 핵심 내용\n",
    "\n",
    "1. **인공신경망 개념**\n",
    "   - 생물학적 뉴런에서 인공 뉴런으로의 발전\n",
    "   - 연결주의와 학습 가능성의 핵심 아이디어\n",
    "   - 전통적 프로그래밍과의 패러다임 차이\n",
    "\n",
    "2. **퍼셉트론과 다층 구조**\n",
    "   - 퍼셉트론의 수학적 원리와 학습 알고리즘\n",
    "   - 선형 분리 가능성 문제와 XOR의 한계\n",
    "   - 다층 구조의 필요성과 범용 근사 정리\n",
    "   - 깊은 신경망의 이론적 근거\n",
    "\n",
    "3. **AI 프레임워크 설치**\n",
    "   - 다양한 프레임워크 비교와 PyTorch 선택 이유\n",
    "   - 환경별 설치 방법과 추가 라이브러리\n",
    "   - 개발 환경 추천과 학습 순서\n",
    "\n",
    "### 🚀 다음 강의 예고: 10강\n",
    "\n",
    "**주제**: 신경망 기초 모델 구현\n",
    "- PyTorch 기본 사용법과 텐서 연산\n",
    "- 퍼셉트론과 다층 신경망 직접 구현\n",
    "- 순전파와 역전파 과정 실습\n",
    "- 활성화 함수와 최적화 알고리즘 적용\n",
    "\n",
    "다음 시간에는 실제 코드로 신경망을 구현해보겠습니다! 🎉\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aibasic",
   "language": "python",
   "name": "aibasic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
